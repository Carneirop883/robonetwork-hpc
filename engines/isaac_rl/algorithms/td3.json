{
  "name": "TD3",
  "label": "Twin Delayed DDPG",
  "description": "Off-policy algorithm designed to reduce overestimation bias, strong baseline for continuous control.",
  "type": "off_policy",

  "params": {
    "tau": {
      "type": "float",
      "label": "Target Update Tau",
      "description": "Soft update coefficient for target networks.",
      "min": 0.0001,
      "max": 0.05,
      "default": 0.005
    },

    "policy_delay": {
      "type": "int",
      "label": "Policy Delay",
      "description": "Number of critic updates per actor update.",
      "min": 1,
      "max": 10,
      "default": 2
    },

    "target_policy_noise": {
      "type": "float",
      "label": "Target Policy Noise",
      "description": "Noise added to target policy during critic update.",
      "min": 0.0,
      "max": 1.0,
      "default": 0.2
    },

    "target_noise_clip": {
      "type": "float",
      "label": "Target Noise Clip",
      "description": "Clipping range for target policy noise.",
      "min": 0.0,
      "max": 1.0,
      "default": 0.5
    },

    "replay_buffer_size": {
      "type": "int",
      "label": "Replay Buffer Size",
      "description": "Maximum number of transitions stored in the replay buffer.",
      "min": 10000,
      "max": 50000000,
      "default": 1000000
    },

    "warmup_steps": {
      "type": "int",
      "label": "Warmup Steps",
      "description": "Number of steps collected before training updates start.",
      "min": 0,
      "max": 500000,
      "default": 10000
    }
  }
}

